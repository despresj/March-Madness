---
title             : "Using GLMs to Predict Basketball Games"
shorttitle        : "Using GLMs to Predict Basketball Games"
author: 
  - name          : "Joe Despres"
    corresponding : yes    # Define only one corresponding author
  - name          : "Sabrina Ball"
affiliation:
  - id            : ""
    institution   : "Michigan State University"
keywords          : "keyword"
wordcount         : "X"
bibliography      : ["r-references.bib"]
floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : no
mask              : no
draft             : no
documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
urlcolor: blue
editor_options: 
  chunk_output_type: inline
---

<!-- 

All datasets and scripts associated with this project can be found at https://github.com/despresj/March-Madness 


clone this reposityory with the terminal command :
```zsh
git clone https://github.com/despresj/March-Madness
```

-->

```{r setup, include = FALSE}
library("papaja")
library(tidymodels)
library(patchwork)
library(kableExtra)
theme_set(theme_test())
r_refs("r-references.bib") 

merged <- readr::read_csv("https://raw.githubusercontent.com/despresj/March-Madness/main/data/merged.csv")
team_stats <- readr::read_csv("https://raw.githubusercontent.com/despresj/March-Madness/main/data/team_stats.csv")
```


## Introduction

Every year qualifying Division I basketball teams compete in the annual March Madness tournament. Casual fans and enthusiasts submit predictions gambling small sums of money in hopes of completing a perfect bracket. A perfect bracket is when one correctly predicts the outcome of all 63 games. This study uses logistic, poisson, and multinomial regression models fitted with R [@R-base] to predict the outcomes of the March Madness tournament  games^[Scripts and datasets assocaited with this project are published in this [Github Repository](https://github.com/despresj/March-Madness)]. Our primary objective is to determine which of these GLMs make the most accurate predictions. To begin, we collect data from three different sources Kaggle [@R-Kaggle], NCAA [@R-NCAA], and the tournament results [@R-NCAA]. Kaggle [@R-Kaggle], provides a comprehensive dataset including all NCAA in-season basketball games from 2001 to 2020. The NCAA [@R-NCAA] provides team-level statistics for each team. We filter, clean, and combine these data using the tidyverse package [@R-tidyverse]. Then use the combination of these datasets to fit our models. The objective is to predict the individual game outcomes as accurately as possible and determine which GLM is the most accurate.  
Our response terms is the outcome of the game, *win or loss* without the possibility of a tie. Our aim is to derive a function that will accurately predict this using team-level statistics. In our dataset, we have many predictors however, we only selected ones that are not co-linear. First, *field goal percentage* which is the ratio of attempted scores to successful scores. *Free-throw percentage*, is the the rate of scoring a penalty shot. Cumulative *Three-point goals made*, field goals made from a sufficient distance. *Rebounds per game*, counts the amount of times a team recovers the ball after a missed shot. *Steals* is when a team was able to remove possession of the ball from the opposing team. *Turnover*, is the number of times the team lost possession. *Blocks*, is the number of times the team was able to block a shot made by the opposing team.  
March Madness is a winner-take-all tournament and teams do not have a second chance to play a game. Therefore, making accurate predictions will be dependent on predicting the previous round correctly. We did make predictions using that method, however our models will be compared by treating the games as independent of the previous round. Meaning, we will filter our data down to include only the 64 teams that qualify, predict every single one of the ${64\choose 2} = 2016$ possible combination, then use the 63 games played as a sample from the population of all possible games. After that, we use basic statistical methods to determine if the predictions were better than random chance, better than betting markets, and better than seeds. Then we use the results to determine which regression model performs the best.  
We will employ three regression methods, logistic, Poisson, and multinomial, to generate predictive models, compare the results, and decide which is best suited to this problem. Logistic regression most naturally suits this problem because games cannot tie and there is a perfectly equal number of wins and losses. This problem could also be suited to Poisson regression because the number of points scored is roughly Poisson distributed. After fitting a Poisson model, we will predict the number of points scored by Team A, and Team B. Then select the team with the highest predicted score to be the predicted winner. A multinomial model is less naturally suited to this problem, however, it may address the shortcomings of the logistic model. In particular, many games are close scoring and won by merely a few points. Therefore, we will use multinomial regression to predict the difference in-game score by assigning the differences into five categories. From there, assign a predicted winner based on which outcome the model considers to be the most likely.

These models predict the correct outcomes with an accuracy between 59% and 65%. GLMs are accurate relative to randomly guessing March Madness tournament outcomes. Team-level statistics, recorded by the NCAA, are highly statistically significant and helpful in predicting the tournament outcomes but do not yield fantastic results. Also, when predicting the probability of a win or a loss using a logit link, the coefficients are symmetric because basketball games are a zero-sum-contest. From our results, we see that logistic and Poisson models perform the best at predicting wins and losses. All three GLM models are well over 50% accurate, therefore, we claim that using GLM models are more accurate than random chance.

## Exploratory Data Analysis  


```{r fig.height=2}
hists <- merged %>% 
  select(x3fg, fg_percent, rpg, st, to,  bkpg)

stat <- function(x, df = hists, rounding_digits = 2) {
    x <- enquo(x)
    df %>%
      summarise(Mean = mean(!!x),
              Median = median(!!x),
                 Std = sd(!!x),
                 Min = min(!!x),
                 Max = max(!!x),
               Range = max(!!x) - min(!!x)) %>%
      mutate_if(is.numeric, round, rounding_digits)
}

variables <- c(
"Three-Points goals Scored",
"Field Goals Scored Percentage",
"Rebounds Per Game",
"Steels Per Game",
"Turn Overs",
"Blocks Per game")

hists %>% 
  map_df(stat) %>% 
  mutate(Variable = variables , .before = Mean) %>% 
  kable(
  format = "latex",
  booktabs = TRUE,
  escape = FALSE,
  longtable = TRUE,
  caption = "Descriptive Statistics")
```

```{r, fig.align="center"}
histogram <- function(x, t) {
  ggplot(data = hists, aes(x)) + 
    geom_histogram(bins = 50, color = "black", fill = "white") + 
    theme_light(base_size = 7) + 
    labs(title = paste0(t), x = "", y = "") + 
    NULL
}

a <- histogram(merged$x3fg, t = "Three-Points Goals")
b <- histogram(merged$fg_percent, t = "Percentage of Field Goals Scored")
c <- histogram(merged$rpg, t = "Rebounds Per Game")
d <- histogram(merged$st, t = "Steals Per Game")
e <- histogram(merged$to, t = "Turn Overs Per Game")
f <- histogram(merged$bkpg, t = "Blocks Per game")
(a + b + c + d + e + f)
```

As mentioned above, there are substantially more variables in the dataset than we used, however, we omitted co-linear predictors. When deciding on which to keep, we selected the variable using deviance and likelihood ratio tests. For our modeling and prediction, we are going to use the statistics associated with both teams. Basic descriptive statistics can be found in Table 1. We notice that the mean of the predictor is relatively close to the median. The standard deviations can be high, but not substantially high, and the ranges are reasonable for the data we have.  As shown in Figure 1, we remark on the predictors being roughly normally distributed with no substantial skew, heavy tails, or slow decay.. 

Now we turn our attention to the outcomes. The logistic model is straight forward, only needing a win to be coded as 1 and a loss to be coded as 0. We fit a Poisson model by taking a score as a count. This is not perfectly suited to Poisson because a winning team's score has a mean of `r mean(merged$w_score) %>% round(2)` and a variance of `r var(merged$w_score) %>% round(2)`. Regardless, we will see how it performs by predicting the score for Team A and Team B, then selecting the one with the highest predicted score as the predicted winner.

```{r warning=FALSE, fig.align="center"}
w <- merged %>% 
  ggplot(aes(x = w_score)) + 
  geom_histogram(bins = 50, color = "black", fill = "white") +
  geom_vline(xintercept = mean(merged$w_score), color = "red", size = 0.25) + 
   labs(title = "Winning Team's Score", x = "", y = "",
       subtitle = paste0("Mean = ", mean(merged$w_score) %>% round(2),
                         " Variance = ", var(merged$w_score) %>% round(2)))
  
l <- merged %>% 
  ggplot(aes(x = l_score)) + 
  geom_histogram(bins = 50, color = "black", fill = "white")  +
  geom_vline(xintercept = mean(merged$l_score), color = "red", size = 0.25) + 
  labs(title = "Losing Team's Score", x = "", y = "",
       subtitle = paste0("Mean = ", mean(merged$l_score) %>% round(2), 
                        " Variance = ", var(merged$l_score) %>% round(2)))
      
  
d <-  merged %>% 
  ggplot(aes(x = diff)) + 
  geom_histogram(bins = 100, color = "black", fill = "white") + 
  geom_vline(xintercept = c(-12, -4, 4, 12), color = "red", size = 0.25) +
  labs(title = "Differences in Score", x = "", y = "") + 
  scale_x_continuous(limits = c(-50, 50), 
                     breaks = c(-28, -20, -12, -4, 4, 12, 20, 28))

(w + l) / d
```

The multinomeal case is not suited to this problem, however, we can adapt it. Commonly seen in betting markets, we will take a look at the difference in score between winner and loser. Then assign it into five categories using the quintiles as shown in Figure 2. The first category is the probability of the team losing by more than twelve points. Second, the probability of the team losing between twelve and four points. Third, a very close game either losing by four or less or winning by four or less (this is fairly common as there is fierce competition between the teams). Fourth, winning by more than four and less than twelve points. In the fifth category, the probability of winning by more than twelve points. We will use the probabilities to predict a winner by comparing the sum of the predicted probabilities of being in the first and second quintiles to being in the fourth and fifth quintiles.  

## Description

To make an accurate comparison, we use the same formula for the three models with only the dependent variable being different. Win or loss for our logistic model, the amount Team A scores for the Poisson model, and the difference in score for the multinomeal model. We tried normalizing the data by subtracting the mean and dividing by the standard deviation for all predictors. However, that did not affect the prediction accuracy. Also, we individually tested each predictor using the likelihood ratio test before adding each term. When we were adding terms, we found many to be co-linear. When we found co-linear predictors, we omitted the one with a smaller likelihood ratio statistic.  

```{r, out.width="100%", fig.cap = ""}
knitr::include_graphics("/Users/josephdespres/Documents/MSU/STT-864_Statistical_Methods_II/March-Madness/paper/paper_files/equ.png")
```

## Results

In Table 2, made with the assistance of the kableExtra package [@R-kableExtra], you can see that we have the results for the Poisson, logistic, and multinomial models. We have a sample of over 35,000 and all of the coefficients are highly significant, which is to be expected because these are the statistics that the NCAA collects as the metrics useful in measuring a team's ability to win games. This study aims to compare predictive models, so we will not cover it exhaustively or include individual z-statistics and p-values. First, the case of the logistic and multinomial models we see that when comparing factors that affect a team's probability of winning, the associated coefficient is nearly equal to the factor capturing the opposing team's metric. That is because basketball is a zero-sum-game, anything good for team A is proportionately bad for team B. Note this is not true for the poisson model because that is measuring points scored rather than estimating probabilities. Take three pointers in the Poisson model, for instance, where an opposing team scores a lot of three pointers has a significant coefficient for the amount of points scored. More points scored is not deterministic of winning, however, it is an indicator.
  
```{r models}
predictors <- "x3fg + opposingx3fg + # field goal pct
              fg_percent + opposingfg_percent + # free throws
              ft_percent + opposingft_percent +# rebound per game
              rpg + opposingrpg + # steels
              st + opposingst + #turnover
              to +  opposingto + # blocks
              opposingbkpg + bkpg"

logistic_fit <- glm(paste0("win ~ ", predictors), data = merged, family = "binomial")
fit_poisson <- glm(paste0("team_score ~ ", predictors), data = merged, family = poisson(link = "log"))
fit_multinom <- VGAM::vglm(paste0("x_tile ~ ", predictors), family = VGAM::cumulative(parallel=TRUE), data = merged)
```

```{r}
row_name <- c("Three Pointers", "O* Three Pointers", "Field Goals", "O* Field Goals",
          "Free-throws", "O* Free-throws", "Rebounds", "O* Rebounds", "Steals", "O* Steals",
          "Turnovers", "O* Turnovers", "Blocks", "O* Blocks")

binder <- function(obj){
  obj %>% tidy() %>% filter(term != "(Intercept)")
}

multi_terms <- c('<-12', '-12:-4', '-4:4', '4:13', row_name)

logistic_pos <- binder(logistic_fit) %>% 
  rename_all(~paste0("L", .x)) %>% 
  bind_cols(binder(fit_poisson) %>% rename_all(~paste0("p", .x))) %>% 
  transmute(`Logistic Model` = Lestimate, `Poisson Model` = pestimate)

tibble(`Logistic Model` = rep(NA, 4),`Poisson Model` = rep(NA, 4)) %>% 
  bind_rows(logistic_pos) %>% 
  bind_cols(tibble(`Multinomial Model` = fit_multinom@coefficients), 
            tibble(Terms = multi_terms)) %>% 
  select(Terms, `Multinomial Model`, `Logistic Model`:`Poisson Model`) %>% 
  mutate_if(is.numeric, round, 5) %>% 
  mutate_at(vars(Terms:`Poisson Model`), replace_na, '--') %>% 
  kable(format = "latex", booktabs = TRUE,
  escape = FALSE,longtable = TRUE, caption = "Regression Output") %>% 
  footnote(number = c("Sample size 35248 games.",
                      "O* is the term asscoiated with the opposing team.",
                      "All the above terms are significant at P < 0.01.",
                      "Multinomeal Intercepts are differences in predicted score."))
  
```


## Goodness of Fit

Now that we have fitted the model, let us take a look at performance. Using the tidyr [@R-tidyr] package, we make every combination of teams and associated statistics in one dataframe. We wrote custom functions tha take two of the 64 teams as inputs and output the probability of winning, predicted amount of points scored, or probabilities of the score resulting in one of the above-mentioned multinomial categories. After that, we used the purrr [@R-purrr] package to iterate over all possible games that could be played. From there, wrote and ran a web scraping script to obtain a dataframe of the tournament results. Finally, we counted the games each model predicted correctly and incorrectly. In this case, our population is all ${64\choose 2}$ possible games in this tournament and a sample is the 63 games that were played. We are not assuming that we have a random sample from the whole population as these are the best teams in the league. Also, there is a selection bias towards games that were played. For example, the teams that played in the finals are represented 6 times in our sample and 32 of the 64 teams are represented only once. Therefore, we do not claim that these results will hold for all NCAA games. However, for the purpose of comparing models that make predictions for this specific tournament is appropriate.

```{r}
bar <- function (obj, title, sub) {
  obj %>% 
  drop_na() %>% 
  ggplot(aes(as.factor(outcome), fill = outcome)) + 
  geom_bar(alpha = 0.7) + 
  labs(title = title, subtitle = paste("Accuracy:", sub, "%"), x = "", y = "") + 
  scale_fill_manual(name = "", values = c("grey20", "grey80"))  +
  geom_text(size=5, color = "black", stat = 'count',
            aes(label =..count.., hjust = 1.5)) + 
  coord_flip()
}

multinomeal_model_score <- readRDS(here::here("cache", "multinomeal_model_score.RDS"))
posson_model_score <- readRDS(here::here("cache","posson_model_score.RDS"))
logistic_model_score <- readRDS(here::here("cache", "logistic_model_score.RDS")) %>% 
  mutate(outcome = if_else(winner == predicted_winner, "Correct", "Incorrect"))

a <- bar(obj = posson_model_score, title = "#1 Poisson Model", sub = "64.5")
b <- bar(obj = logistic_model_score, title = "#2 Logistic Model", sub = "63.1")
c <- bar(obj = multinomeal_model_score , title = "#3 Multinomeal Model", sub = "59.6")

a/b/c
```

## Conculsion

From these results, we see that the logistic and Poisson models outperform the multinomial. All three models are well over 50% accurate, confirming the claim that GLMs can predict March Madness outcomes better than random chance. Also, we confirmed the suspicion that basketball statistics are symmetric in relation to the probability of winning and are not when we are predicting the game score.

The limits of this study have a lot to do with data limitations. Ideally, we would have more tournaments to test on with lower-skilled teams. Also, the results would be much more robust if the tournament was larger. We had a lot of data points over 35,000, which is more than sufficient, however, we do not have an abundance of non-co-linear covariates. An accuracy of 64.5% is not superb considering betting markets take these exact factors into account, however, being higher than random chance indicates that GLMs are useful in predicting March Madness outcomes. 

Future studies could be focused on making a better fit and getting a better understanding of season games before making predictions like this. To get more robust predictions, it would be beneficial to go back through the season games data and draw out the times each team in the March Madness tournament played each other and test our model against that. While exploring, we found that teams from different parts of the country tended to have a different coefficient on statistics and outcomes. A team's conferences are likely a random effect that should be taken into account. Therefore, additional research into the random effect would certainly yield more robust predictions.


\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.0in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

```{r}
beepr::beep()
```

